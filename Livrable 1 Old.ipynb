{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Groupe non officiel 1\n",
    "# Livrable 1 - Classification binaire\n",
    "\n",
    "|Auteur|Centre|\n",
    "|---|---|\n",
    "|DIMEGLIO Nicolas|Aix-en-Provence|\n",
    "|ROMANO Sébastien|Aix-en-Provence|\n",
    "|SIXDENIER Alexandre|Aix-en-Provence|\n",
    "|VESSERON Alexandre|Aix-en-Provence|\n",
    "\n",
    "# Rappel du sujet\n",
    "L'entreprise voulant automatiser la sélection de photos pour l'annotations, le but est de fournir une méthode de classification binaire afin de filtrer les images qui ne sont pas des photos du dataset de départ. Pour ce faire nous allons nous appuyer sur l'architecture des réseau de neurones, ainsi que l'analyse des résultats obtenus.\n",
    "Toutes les parties doivent être détaillée dans le notebook :\n",
    "    - les paramètre du réseau,\n",
    "    - la fonction de perte ainsi que l'algorithme d'optimisation utilisé pour l’entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyse à effectuer\n",
    "- Graphique contenant l'évolution de l'erreur d’entraînement ainsi que de l'erreur de test\n",
    "- Graphique l'évolution de l'accuracy pour ces deux datasets.\n",
    "\n",
    "- L'analyse de ces résultats, notamment le compromis entre biais et variance (ou sur-apprentissage et sous-apprentissage).\n",
    "\n",
    "- Une description des méthodes potentiellement utilisables pour améliorer les compromis biais/variance : technique de régularisation, drop out, early-stopping, ...\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import des différentes bibliothèques"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import zipfile\n",
    "import os\n",
    "import PIL\n",
    "import subset as subset\n",
    "import tensorflow as tf\n",
    "from PIL import Image as Image\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Téléchargement des données et dezippage"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "path_to_zip_file = \"Dataset projet\" #\"C:\\\\Users\\\\nicos\\\\PycharmProjects\\\\Projet_DataScience_A5\\\\Dataset projet\"\n",
    "directory_to_extract_to = \"C:\\\\Users\\\\nicos\\\\PycharmProjects\\\\Projet_DataScience_A5\\\\Dataset\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Livrable 1 - Painting.zip\n",
      "Dataset Livrable 1 - Photo.zip\n",
      "Dataset Livrable 1 - Schematics.zip\n",
      "Dataset Livrable 1 - Sketch.zip\n",
      "Dataset Livrable 1 - Text.zip\n"
     ]
    }
   ],
   "source": [
    "data_dir = directory_to_extract_to\n",
    "for directory in os.listdir(path_to_zip_file):\n",
    "    print(directory)\n",
    "    with zipfile.ZipFile(path_to_zip_file + \"\\\\\" + directory, 'r') as zip_ref:\n",
    "        zip_ref.extractall(directory_to_extract_to)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicos\\PycharmProjects\\Projet_DataScience_A5\\Dataset\\Painting\n",
      "C:\\Users\\nicos\\PycharmProjects\\Projet_DataScience_A5\\Dataset\\Photo\n",
      "C:\\Users\\nicos\\PycharmProjects\\Projet_DataScience_A5\\Dataset\\Schematics\n",
      "C:\\Users\\nicos\\PycharmProjects\\Projet_DataScience_A5\\Dataset\\Sketch\n",
      "C:\\Users\\nicos\\PycharmProjects\\Projet_DataScience_A5\\Dataset\\Text\n"
     ]
    }
   ],
   "source": [
    "for directory in os.listdir(directory_to_extract_to):\n",
    "    print(directory_to_extract_to + \"\\\\\" + directory)\n",
    "    for file in os.listdir(directory_to_extract_to + \"\\\\\" + directory):\n",
    "        image = None\n",
    "        try:\n",
    "            image = Image.open(directory_to_extract_to + \"\\\\\" + directory + \"\\\\\" + file)\n",
    "            data = list(image.getdata())\n",
    "            image_without_exif = Image.new(image.mode, image.size)\n",
    "            image_without_exif.putdata(data)\n",
    "            image_without_exif.save(directory_to_extract_to + \"\\\\\" + directory + \"\\\\\" + file)\n",
    "        except:\n",
    "            print(directory_to_extract_to + \"\\\\\" + directory + \"\\\\\" + file)\n",
    "            print(image)\n",
    "            image.close()\n",
    "            os.remove(directory_to_extract_to + \"\\\\\" + directory + \"\\\\\" + file)\n",
    "            pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Récupération des données"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(data_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour commencer, nous devons spécifier quelques paramètres pour l'apprentissage:\n",
    "<ul>\n",
    "    <li>La longueur et la largeur des images. </li>\n",
    "    <li>La taille du batch.</li>\n",
    "</ul>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_h = 180\n",
    "image_w = 180\n",
    "batch_s = 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Préparation des données"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous devons partager le jeu de données en jeu d'entrainement et de validation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Le train_set\n",
    "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split= 0.20,\n",
    "  subset = 'training',\n",
    "  seed=42,\n",
    "  image_size=(image_h, image_w),\n",
    "  batch_size=batch_s,\n",
    "  color_mode='rgb',\n",
    "  label_mode='int',\n",
    "  labels=\"inferred\"\n",
    ")\n",
    "\n",
    "# Le test_set\n",
    "test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split= 0.20,\n",
    "  subset = 'validation',\n",
    "  seed=42,\n",
    "  image_size=(image_h, image_w),\n",
    "  batch_size=batch_s,\n",
    "  color_mode='rgb',\n",
    "  label_mode='int',\n",
    "  labels=\"inferred\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La sortie nous permet de voir le nombres de fichier total et de savoir de combien de fichier sont constitués nos jeux d'entrainement et de test.\n",
    "\n",
    "# 2. Exploration et visualisation des données\n",
    "Commençons, tout d'abord par afficher le nom des différentes classes de nos données."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_names =  train_set.class_names #A COMPLETER\n",
    "print(class_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Afin de vérifier la bonne récupération des données, nous allons afficher quelques images ranndom issues de tout nos dossiers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for images, labels in train_set.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On voudrait aussi connaître la taille des données, afin de gérer les performances du modèle."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_set))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Configuration de l'environnement pour l'entrainement\n",
    "\n",
    "- `Dataset.cache()` : Cette fonction sert à forcer le maintien des données en cache dans la mémoire. Vu que le réseau de neurones fait plusieurs passes (qu'on nomme _époque_ ou _epoch_ en anglais) sur les données durant l'apprentissage, cette fonction permet de ne pas avoir à recharger les images à chaque fois.\n",
    "- `Dataset.prefetch()` : Cette fonction permet de faire le prétraitement de l'élément courant du jeu de données (par exemple le batch suivant) en même temps que l'entrainement/évaluation du batch courant par le modèle. Dans un environnement multi-processeurs ou multi-cœur, c'est un gain de temps non négligeable."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_set = train_set.cache().shuffle(5).prefetch(buffer_size=AUTOTUNE)\n",
    "test_set = test_set.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. La normalisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Le modèle de réseau de neurones convolutif (CNN)\n",
    "\n",
    "On va par la suite créer un modèle vide à l'aide de la fonction Sequential de tensorflow.\n",
    "Par la suite nous rajouterons la classification d'image.\n",
    "Pour être sur que notre modèle soit bien entrainer, nous allons utiliser des techniques de régularisation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_classes = 5 # Nombre de classes et donc aussi nombre de neurones dans la dernière couche\n",
    "model = models.Sequential()\n",
    "model = tf.keras.layers.Dropout(\n",
    "    rate=0.2, noise_shape=None, seed=42,\n",
    ")\n",
    "# Résumé du modèle\n",
    "model = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\n",
    "        mode=\"horizontal_and_vertical\",seed=42,input_shape=(image_h, image_w, 3)),\n",
    "    layers.experimental.preprocessing.RandomRotation(\n",
    "        factor = (-0.2, 0.3),\n",
    "        fill_mode=\"reflect\",\n",
    "        interpolation=\"bilinear\",\n",
    "        seed=42,\n",
    "        fill_value=0.0,\n",
    "    ),\n",
    "    layers.experimental.preprocessing.RandomZoom(\n",
    "        height_factor= (0.2, 0.3),\n",
    "        width_factor=None,\n",
    "        fill_mode=\"reflect\",\n",
    "        interpolation=\"bilinear\",\n",
    "        seed=42,\n",
    "        fill_value=0.0,\n",
    "    ),\n",
    "  tf.keras.layers.experimental.preprocessing.Rescaling(scale=1./255, offset=0),\n",
    "  tf.keras.layers.Conv2D(16, [3,3], activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, [3,3], activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, [3,3], activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On compile le modèle."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',  #A COMPLETER\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Après avoir compiler le model, nous allons entrainer notre réseau de neuronne avec les données normalisées."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs=10\n",
    "\n",
    "history = model.fit(train_set,validation_data = test_set,epochs=epochs)\n",
    "model.summary()\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Supression des données à tester"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#for directory in os.listdir(path_to_zip_file):\n",
    "#   with zipfile.ZipFile(path_to_zip_file + \"\\\\\" + directory, 'r') as zip_ref:\n",
    "#            os.remove(directory)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Groupe non officiel 1\n",
    "# Livrable 1 - Classification binaire Création du model\n",
    "\n",
    "|Auteur|Centre|\n",
    "|---|---|\n",
    "|DIMEGLIO Nicolas|Aix-en-Provence|\n",
    "|ROMANO Sébastien|Aix-en-Provence|\n",
    "|SIXDENIER Alexandre|Aix-en-Provence|\n",
    "|VESSERON Alexandre|Aix-en-Provence|\n",
    "\n",
    "# Rappel du sujet\n",
    "L'entreprise voulant automatiser la sélection de photos pour l'annotations, le but est de fournir une méthode de classification binaire afin de filtrer les images qui ne sont pas des photos du dataset de départ. Pour ce faire nous allons nous appuyer sur l'architecture des réseau de neurones, ainsi que l'analyse des résultats obtenus.\n",
    "Toutes les parties doivent être détaillée dans le notebook :\n",
    "    - les paramètre du réseau,\n",
    "    - la fonction de perte ainsi que l'algorithme d'optimisation utilisé pour l’entraînement."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyse à effectuer\n",
    "- Graphique contenant l'évolution de l'erreur d’entraînement ainsi que de l'erreur de test\n",
    "- Graphique l'évolution de l'accuracy pour ces deux datasets.\n",
    "\n",
    "- L'analyse de ces résultats, notamment le compromis entre biais et variance (ou sur-apprentissage et sous-apprentissage).\n",
    "\n",
    "- Une description des méthodes potentiellement utilisables pour améliorer les compromis biais/variance : technique de régularisation, drop out, early-stopping, ...\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import des différentes bibliothèques"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import zipfile\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Téléchargement des données et dezippage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "path_to_zip_file = \"Dataset projet\" #\"C:\\\\Users\\\\nicos\\\\PycharmProjects\\\\Projet_DataScience_A5\\\\Dataset projet\"\n",
    "directory_to_extract_to = \"C:\\\\Users\\\\nicos\\\\PycharmProjects\\\\Projet_DataScience_A5\\\\Dataset\"\n",
    "data_dir = directory_to_extract_to"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Récupération des données"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(data_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour commencer, nous devons spécifier quelques paramètres pour l'apprentissage:\n",
    "<ul>\n",
    "    <li>La longueur et la largeur des images. </li>\n",
    "    <li>La taille du batch.</li>\n",
    "</ul>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "image_h = 256\n",
    "image_w = 256\n",
    "batch_s = 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Préparation des données"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous devons partager le jeu de données en jeu d'entrainement et de validation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\nicos\\\\PycharmProjects\\\\Projet_DataScience_A5\\\\Dataset'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16528/1344981184.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m   \u001B[0mcolor_mode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'rgb'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m   \u001B[0mlabel_mode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'int'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m   \u001B[0mlabels\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"inferred\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m )\n\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\document\\ecole\\cesi\\annee 5\\projet_datascience_a5\\venv\\lib\\site-packages\\keras\\preprocessing\\image_dataset.py\u001B[0m in \u001B[0;36mimage_dataset_from_directory\u001B[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001B[0m\n\u001B[0;32m    195\u001B[0m       \u001B[0mshuffle\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mshuffle\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    196\u001B[0m       \u001B[0mseed\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mseed\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 197\u001B[1;33m       follow_links=follow_links)\n\u001B[0m\u001B[0;32m    198\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    199\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mlabel_mode\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'binary'\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclass_names\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\document\\ecole\\cesi\\annee 5\\projet_datascience_a5\\venv\\lib\\site-packages\\keras\\preprocessing\\dataset_utils.py\u001B[0m in \u001B[0;36mindex_directory\u001B[1;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001B[0m\n\u001B[0;32m     64\u001B[0m   \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m     \u001B[0msubdirs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 66\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0msubdir\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msorted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlistdir\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdirectory\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     67\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misdir\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdirectory\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msubdir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m         \u001B[0msubdirs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msubdir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\nicos\\\\PycharmProjects\\\\Projet_DataScience_A5\\\\Dataset'"
     ]
    }
   ],
   "source": [
    "# Le train_set\n",
    "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split= 0.20,\n",
    "  subset = 'training',\n",
    "  seed=42,\n",
    "  image_size=(image_h, image_w),\n",
    "  batch_size=batch_s,\n",
    "  color_mode='rgb',\n",
    "  label_mode='int',\n",
    "  labels=\"inferred\"\n",
    ")\n",
    "\n",
    "# Le test_set\n",
    "test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split= 0.20,\n",
    "  subset = 'validation',\n",
    "  seed=42,\n",
    "  image_size=(image_h, image_w),\n",
    "  batch_size=batch_s,\n",
    "  color_mode='rgb',\n",
    "  label_mode='int',\n",
    "  labels=\"inferred\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La sortie nous permet de voir le nombres de fichier total et de savoir de combien de fichier sont constitués nos jeux d'entrainement et de test.\n",
    "\n",
    "# 2. Exploration et visualisation des données\n",
    "Commençons, tout d'abord par afficher le nom des différentes classes de nos données."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16528/1923537721.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mclass_names\u001B[0m \u001B[1;33m=\u001B[0m  \u001B[0mtrain_set\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclass_names\u001B[0m \u001B[1;31m#A COMPLETER\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclass_names\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "class_names =  train_set.class_names #A COMPLETER\n",
    "print(class_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Afin de vérifier la bonne récupération des données, nous allons afficher quelques images ranndom issues de tout nos dossiers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for images, labels in train_set.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On voudrait aussi connaître la taille des données, afin de gérer les performances du modèle."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_set))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Configuration de l'environnement pour l'entrainement\n",
    "\n",
    "- `Dataset.cache()` : Cette fonction sert à forcer le maintien des données en cache dans la mémoire. Vu que le réseau de neurones fait plusieurs passes (qu'on nomme _époque_ ou _epoch_ en anglais) sur les données durant l'apprentissage, cette fonction permet de ne pas avoir à recharger les images à chaque fois.\n",
    "- `Dataset.prefetch()` : Cette fonction permet de faire le prétraitement de l'élément courant du jeu de données (par exemple le batch suivant) en même temps que l'entrainement/évaluation du batch courant par le modèle. Dans un environnement multi-processeurs ou multi-cœur, c'est un gain de temps non négligeable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_set = train_set.cache().shuffle(5).prefetch(buffer_size=AUTOTUNE)\n",
    "test_set = test_set.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. La normalisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Le modèle de réseau de neurones convolutif (CNN)\n",
    "\n",
    "On va par la suite créer un modèle vide à l'aide de la fonction Sequential de tensorflow.\n",
    "Par la suite nous rajouterons la classification d'image.\n",
    "Pour être sur que notre modèle soit bien entrainer, nous allons utiliser des techniques de régularisation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Résumé du modèle\n",
    "def create_model(summary = False, num_classes = 5, input_shape=(28,28,3),\n",
    "                 loss_fn_to_use = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.experimental.preprocessing.RandomFlip(\n",
    "            mode=\"horizontal_and_vertical\",seed=42,input_shape=(image_h, image_w, 3)),\n",
    "        layers.experimental.preprocessing.RandomRotation(\n",
    "            factor = (-0.2, 0.3),\n",
    "            fill_mode=\"reflect\",\n",
    "            interpolation=\"bilinear\",\n",
    "            seed=42,\n",
    "            fill_value=0.0,\n",
    "        ),\n",
    "        layers.experimental.preprocessing.RandomZoom(\n",
    "            height_factor= (0.2, 0.3),\n",
    "            width_factor=None,\n",
    "            fill_mode=\"reflect\",\n",
    "            interpolation=\"bilinear\",\n",
    "            seed=42,\n",
    "            fill_value=0.0,\n",
    "        ),\n",
    "      tf.keras.layers.experimental.preprocessing.Rescaling(scale=1./255, offset=0),\n",
    "      tf.keras.layers.Conv2D(64, [3,3], stride = (1,1), input_shape = input_shape, padding= \"valid\",activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(pool_size = (2,2), stride = (1,1),padding = \"same\"),\n",
    "      tf.keras.layers.Conv2D(32, [5,5], stride = (2,2), input_shape = input_shape, padding= \"valid\",activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(pool_size = (2,2), stride = (1,1),padding = \"same\"),\n",
    "      tf.keras.layers.Flatten(input_shape = (28,28)),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dropout(\n",
    "        rate=0.2, noise_shape=None, seed=42,\n",
    "        ),\n",
    "      tf.keras.layers.Dense(num_classes)\n",
    "    ])\n",
    "    if summary:\n",
    "        model.summary()\n",
    "    #Set Loss Function\n",
    "    loss_fn=loss_fn_to_use\n",
    "    #On compile le modèle.\n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss = loss_fn,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Après avoir compiler le model, nous allons entrainer notre réseau de neuronne avec les données normalisées."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs=10\n",
    "model = create_model(summary = True)\n",
    "\n",
    "history = model.fit(train_set,validation_data = test_set,epochs=epochs)\n",
    "model.summary()\n",
    "model.save_weights('./test_model')\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Supression des données à tester"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#for directory in os.listdir(path_to_zip_file):\n",
    "#   with zipfile.ZipFile(path_to_zip_file + \"\\\\\" + directory, 'r') as zip_ref:\n",
    "#            os.remove(directory)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}